{"cells":[{"cell_type":"markdown","metadata":{},"source":["<h1><b><font size = 10>Chatbot using Python Phase-5</font></b></h1>"]},{"cell_type":"markdown","metadata":{},"source":["### Team Members:\n","College Name : Madras Institute of technology, Anna university\n","\n","Team Members :\n","\n","| Name   | NM ID | email                   |\n","|---------------|-----------|-------------------------|\n","| Som Sridhar Roy      | 346D3A8D29E50409E7A4DB2289604BD2| somsridharroy07j@gmail.com |\n","| Sachin Sambandha Saravana  | 08827AD8016DDD1A83141F3958069E92  | sachinsarvana004@gmail.com    |\n","| Prince Alexander George |F5CFA2A6D9BA714B5134D0D8F3A80795| princealex2212@gmail.com|\n","| Varun D | 1A39BFD15A49F27965B4F30196A921B5| dvarun2003@gmail.com  |"]},{"cell_type":"markdown","metadata":{},"source":["# Problem Statement\n","\n","Traditional customer service operations have long been characterized by substantial resource allocation, both in terms of manpower and financial investments. The conventional approach to customer support, often reliant on extensive call centers and large support teams, has proven to be costly and, at times, inefficient. This situation has prompted the need for innovative solutions that can optimize resource utilization while maintaining or even enhancing the quality of customer service.\n","\n","To tackle this challenge, we seek to leverage the remarkable advancements in Language Model Models (LLMs). These cutting-edge language models have revolutionized the field of natural language understanding and generation. By harnessing the power of LLMs, this project endeavors to introduce a highly adaptable and intelligent chatbot into the realm of customer service. This chatbot will not only serve as a cost-effective alternative but also exhibit the capability to cater to the needs of customers in a manner that is both efficient and effective.\n","The goal is to transform the customer service landscape by harnessing the potential of LLMs to create a sophisticated chatbot that can provide timely and personalized assistance, ultimately reducing resource overheads while simultaneously elevating the overall customer experience.\n"]},{"cell_type":"markdown","metadata":{},"source":["# Design Thinking\n","\n","1. **Functionality:**\n","   - The chatbot will employ advanced Natural Language Processing (NLP) techniques to provide clear and accurate responses to customer queries. It will offer customization options, enabling companies to fine-tune its responses to align with their specific customer service requirements.\n","\n","2. **User Interface:**\n","   - The chatbot will be designed to offer a user-friendly interface for customers to engage with.\n","\n","3. **Natural Language Processing (NLP):**\n","   - The chatbot will implement cutting-edge NLP algorithms to understand and interpret customer inquiries, ensuring it can effectively address a wide range of questions and issues.\n","\n","4. **Responses:**\n","   - The chatbot will have the capability to offer various types of assistance, including providing solutions to common problems, contacting the company on behalf of the user, and offering information on company products or services.\n","\n","5. **Testing and Improvement:**\n","   - The chatbot's responses will continuously evolve based on customer satisfaction feedback. Machine learning techniques, such as reinforcement learning, will be employed to fine-tune the chatbot's performance over time. Rigorous testing procedures will be in place to evaluate its capabilities and identify areas for improvement.\n"]},{"cell_type":"markdown","metadata":{},"source":["We will now proceed to the construction of the model"]},{"cell_type":"markdown","metadata":{},"source":["# Setting Up The Environment"]},{"cell_type":"markdown","metadata":{},"source":["## Creating a Flask Environment and Web Interface for the Chatbot"]},{"cell_type":"markdown","metadata":{},"source":["The Flask app's routes are the endpoints that guide the behavior of a web application. In the context of a chatbot, the routes are configured to create a user-friendly website where users can interact with the chatbot. For example, there's a route for displaying the chat interface and another for handling chatbot responses. When a user accesses the website, they see a chatbox for input. When they send a message, the input is processed, a chatbot response is generated, and the response is displayed within the chat interface. This interaction is facilitated through these Flask routes, enabling seamless communication between the user and the chatbot via a web interface."]},{"cell_type":"markdown","metadata":{},"source":["Flask App :"]},{"cell_type":"raw","metadata":{},"source":["from flask import Flask\n","\n","app = Flask(__name__)\n","@app.route('/')\n","def chat():\n","    return render_template('index.html')\n","\n","@app.route('/get_response', methods=['POST'])\n","def get_response():\n","    # Code to process user input and generate chatbot responses\n","    user_input = request.form['user_input']\n","    chatbot_response = generate_chatbot_response(user_input)\n","    return jsonify({'chatbot_response': chatbot_response})\n","\n","if __name__ == '__main__':\n","    app.run(debug=True)"]},{"cell_type":"markdown","metadata":{},"source":["Web Interface :"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"html"}},"outputs":[],"source":["<!DOCTYPE html>\n","<html>\n","  <title>ChatBot</title>\n","  <head>\n","    <script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js\"></script>\n","    <style>\n","      body {\n","        font-family: monospace;\n","        background-color: black;\n","      }\n","      h1 {\n","        background-color: black;\n","        color: white;\n","        display: inline-block;\n","        font-size: 2em;\n","        margin: 0;\n","        padding: 10px;\n","      }\n","      #chatbox {\n","        margin-left: auto;\n","        margin-right: auto;\n","        width: 55%;\n","        margin-top: 60px;\n","      }\n","      p{\n","        color: white;\n","      }\n","      #userInput {\n","        margin-left: auto;\n","        margin-right: auto;\n","        width: 40%;\n","        margin-top: 60px;\n","      }\n","      #textInput {\n","        width: 90%;\n","        border: none;\n","        border-bottom: 3px white;\n","        font-family: monospace;\n","        font-size: 17px;\n","        margin-bottom: 10px;\n","      }\n","      .userText {\n","        color: white;\n","        font-family: monospace;\n","        font-size: 17px;\n","        text-align: right;\n","        line-height: 30px;\n","        border-radius: 5px;\n","        margin-top: 25px;\n","      }\n","      .userText span {\n","        background-color: black;\n","        padding: 10px;\n","        border-radius: 7px;\n","        border-color: white;\n","      }\n","      .botText {\n","        color: whitesmoke;\n","        font-family: monospace;\n","        font-size: 17px;\n","        text-align: left;\n","        line-height: 30px;\n","        background-color: black ;\n","        border-radius: 7px;\n","        border-color: white;\n","      }\n","      .botText span {\n","        padding: 10px;\n","        border-radius: 2px;\n","      }\n","      #tidbit {\n","        position: absolute;\n","        bottom: 0;\n","        right: 0;\n","        width: 300px;\n","      }\n","      .boxed {\n","        margin-left: auto;\n","        margin-right: auto;\n","        width: 70%;\n","        margin-top: 60px;\n","        background-color: black;\n","      }\n","      .box {\n","        border: 2px solid black;\n","      }\n","    </style>\n","  </head>\n","  <body>\n","    <center>\n","      <h1>\n","        Chatbot In Python\n","      </h1>\n","    </center>\n","    <div class=\"box\"></div>\n","    <div class=\"boxed\">\n","      <div>\n","        <div id=\"chatbox\">\n","          <span><img src = \"https://cdn.iconscout.com/icon/premium/png-256-thumb/ai-robot-5-1089411.png\" alt = \"Hello\" width = \"25\"></span>\n","          <span><p class=\"botText\">\n","            <span>Hi! I'm an AI Chatbot. Ask me anything!</span>\n","          </p></span>\n","        </div>\n","        <div id=\"userInput\">\n","          <input id=\"textInput\" type=\"text\" name=\"msg\" placeholder=\"Enter message here...\" />\n","        </div>\n","      </div>\n","      <script>\n","        function getBotResponse() {\n","          var image = '<img src = \"https://cdn.iconscout.com/icon/premium/png-256-thumb/ai-robot-5-1089411.png\" alt = \"Hello\" width = \"25\">';\n","          var rawText = $(\"#textInput\").val();\n","          var userHtml = '<p class=\"userText\"><span>' + rawText + \"</span></p>\";\n","          $(\"#textInput\").val(\"\");\n","          $(\"#chatbox\").append(userHtml);\n","          document\n","            .getElementById(\"userInput\")\n","            .scrollIntoView({ block: \"start\", behavior: \"smooth\" });\n","          $.get(\"/get\", { msg: rawText }).done(function(data) {\n","            var botHtml =\"<p class=botName>Bot:</p>\"+'<p class=\"botText\"><span>' + data + \"</span></p>\";\n","            \n","            $(\"#chatbox\").append(botHtml);\n","            document\n","              .getElementById(\"userInput\")\n","              .scrollIntoView({ block: \"start\", behavior: \"smooth\" });\n","          });\n","        }\n","        $(\"#textInput\").keypress(function(e) {\n","          if (e.which == 13) {\n","            getBotResponse();\n","          }\n","        });\n","      </script>\n","    </div>\n","  </body>\n","</html>"]},{"cell_type":"markdown","metadata":{},"source":["Finally to execute the chat app simply run"]},{"cell_type":"raw","metadata":{},"source":["python app.py"]},{"cell_type":"markdown","metadata":{},"source":["# The Algorithm Used"]},{"cell_type":"markdown","metadata":{},"source":[" In this project, a sequence-to-sequence (seq2seq) architecture with Bahdanau attention mechanism is employed. The encoder, responsible for processing the input sequence, utilizes a Gated Recurrent Unit (GRU) layer to compress the input into a fixed-size context vector. The attention mechanism, specifically Bahdanau attention, is applied in the decoder. This attention mechanism enables the model to focus on different parts of the input sequence while generating the output, enhancing its ability to handle long and complex sequences effectively. The decoder, also employing a GRU layer, utilizes the attention-weighted context vector along with the previously generated tokens to generate the next token in the output sequence. This comprehensive architecture allows the model to capture intricate patterns and relationships within the input data, making it adept at tasks such as language translation or chatbot responses."]},{"cell_type":"markdown","metadata":{},"source":["# The Dataset Used"]},{"cell_type":"markdown","metadata":{},"source":["The dataset that has been chosen consists of conversational exchanges typical of a chatbot interaction. Each entry contains two alternating lines: a question followed by an answer. The dataset captures a casual and friendly conversation where individuals inquire about each other's well-being, discuss their current activities, and engage in small talk. The dataset covers general, everyday topics and serves as a suitable foundation for training a chatbot to respond in a natural and personable manner.\n","In this dataset, the questions and answers are seperated by a tab and the whole dataset is stored in dialogs.txt\n","\n","Dataset link : https://www.kaggle.com/datasets/grafstor/simple-dialogs-for-chatbot/data"]},{"cell_type":"markdown","metadata":{},"source":["# Preprocessing the Dataset"]},{"cell_type":"markdown","metadata":{},"source":[" Here, we begin by importing essential libraries including `TensorFlow` for machine learning, `Matplotlib` for data visualization, and `scikit-learn` for data splitting. "]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import tensorflow as tf\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","from sklearn.model_selection import train_test_split\n","\n","import unicodedata\n","import re\n","import numpy as np\n","import os\n","import io\n","import time\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{},"source":["The code reads a dataset from 'dialogs.txt', separates dialogues into questions and answers, and displays the first question and its corresponding answer. This code represents the initial steps of data preprocessing in our project."]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[],"source":["file = open('dialogs.txt','r').read()"]},{"cell_type":"code","execution_count":7,"metadata":{"trusted":true},"outputs":[],"source":["qna_list = [f.split('\\t') for f in file.split('\\n')]\n","\n","questions = [x[0] for x in qna_list]\n","answers = [x[1] for x in qna_list]"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Question:  hi, how are you doing?\n","Answer:  i'm fine. how about yourself?\n"]}],"source":["print(\"Question: \", questions[0])\n","print(\"Answer: \", answers[0])"]},{"cell_type":"markdown","metadata":{},"source":["# Preprocessing the sentences"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[],"source":["def unicode_to_ascii(s):\n","    return ''.join(c for c in unicodedata.normalize('NFD', s)\n","      if unicodedata.category(c) != 'Mn')\n","\n","\n","def preprocess_sentence(w):\n","    w = unicode_to_ascii(w.lower().strip())\n","\n","    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n","    w = re.sub(r'[\" \"]+', \" \", w)\n","\n","    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n","    w = w.strip()\n","\n","    w = '<start> ' + w + ' <end>'\n","    return w"]},{"cell_type":"markdown","metadata":{},"source":["The `unicode_to_ascii` function normalizes text by removing diacritics and accents. The preprocess_sentence function further processes text by converting it to lowercase, adding start and end tokens, and removing unwanted characters. "]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<start> hi , how are you doing ? <end>\n","<start> i m fine . how about yourself ? <end>\n"]}],"source":["print(preprocess_sentence(questions[0]))\n","print(preprocess_sentence(answers[0]))\n","\n","pre_questions = [preprocess_sentence(w) for w in questions]\n","pre_answers = [preprocess_sentence(w) for w in answers]"]},{"cell_type":"markdown","metadata":{},"source":["the code preprocesses both questions and answers from the dataset using these functions. It essentially prepares the textual data for further NLP tasks by cleaning and formatting the sentences."]},{"cell_type":"markdown","metadata":{},"source":["# Tokenization"]},{"cell_type":"markdown","metadata":{},"source":["Tokenization is a crucial preprocessing step in chatbot development and NLP tasks. It converts text into numerical tokens, enabling machine learning models to process and understand language. Additionally, tokenization ensures uniform sentence lengths, facilitating efficient model training. It also creates a vocabulary, allowing the model to learn word relationships and generate accurate responses. Overall, tokenization is essential for translating raw text data into a format that machine learning algorithms can work with effectively."]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[],"source":["def tokenize(lang):\n","    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n","      filters='')\n","    lang_tokenizer.fit_on_texts(lang)\n","\n","    tensor = lang_tokenizer.texts_to_sequences(lang)\n","\n","    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n","                                                         padding='post')\n","\n","    return tensor, lang_tokenizer"]},{"cell_type":"markdown","metadata":{},"source":["The `tokenize` function is employed to tokenize the input text sequences, converting words into numerical tokens. This involves initializing a tokenizer, fitting it on the provided text sequences, and subsequently padding the tokenized sequences. Padding ensures that all sequences have a uniform length, which is necessary for effective model training."]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[],"source":["def load_dataset(data, num_examples=None):\n","    # creating cleaned input, output pairs\n","    if(num_examples != None):\n","        targ_lang, inp_lang, = data[:num_examples]\n","    else:\n","        targ_lang, inp_lang, = data\n","\n","    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n","    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n","\n","    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"]},{"cell_type":"markdown","metadata":{},"source":["the `load_dataset` function is used to create cleaned input and output pairs. It takes the tokenized language data and, if specified, selects a subset of examples. In this specific case, it's set to work with 30,000 examples. This function also returns tokenized representations and tokenizer objects for both input and target languages."]},{"cell_type":"code","execution_count":13,"metadata":{"trusted":true},"outputs":[],"source":["num_examples = 30000\n","data = pre_answers, pre_questions\n","input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(data, num_examples)\n","\n","# Calculate max_length of the target tensors\n","max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"]},{"cell_type":"code","execution_count":14,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2980 2980 745 745\n"]}],"source":["# Creating training and validation sets using an 80-20 split\n","input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n","\n","# Show length\n","print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"]},{"cell_type":"markdown","metadata":{},"source":["Furthermore, this code calculates the maximum sequence lengths for both input and target tensors. It then proceeds to split the data into training and validation sets using an 80-20 split ratio, with the lengths of these sets being displayed for reference. This entire process is essential in data preprocessing for training a chatbot model, ensuring that the data is properly tokenized and split into training and validation sets for effective machine learning."]},{"cell_type":"markdown","metadata":{},"source":["# Word to index"]},{"cell_type":"markdown","metadata":{},"source":["The code below defines a function called `convert` to map numerical tokens back to their corresponding words in a given language. It takes a language (vocabulary) and a tensor of tokenized sequences as input. The code then demonstrates this mapping by printing the original words of a selected training example from both the input and target languages, providing an index-to-word conversion for better understanding of the tokenized data."]},{"cell_type":"code","execution_count":17,"metadata":{"trusted":true},"outputs":[],"source":["def convert(lang, tensor):\n","    for t in tensor:\n","        if t!=0:\n","            print (\"%d ----> %s\" % (t, lang.index_word[t]))"]},{"cell_type":"markdown","metadata":{},"source":["This is a crucial step in the NLP pipeline, as it allows for human-readable inspection and verification of the data used in training and model output, ensuring that the tokenization and vocabulary management process has been executed correctly."]},{"cell_type":"code","execution_count":18,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Input Language; index to word mapping\n","1 ----> <start>\n","55 ----> really\n","5 ----> ?\n","15 ----> what\n","210 ----> kind\n","18 ----> of\n","351 ----> shoes\n","21 ----> are\n","22 ----> they\n","5 ----> ?\n","2 ----> <end>\n","\n","Target Language; index to word mapping\n","1 ----> <start>\n","266 ----> these\n","23 ----> are\n","435 ----> called\n","57 ----> all\n","1441 ----> star\n","1442 ----> chuck\n","1443 ----> taylors\n","3 ----> .\n","2 ----> <end>\n"]}],"source":["print (\"Input Language; index to word mapping\")\n","convert(inp_lang, input_tensor_train[0])\n","print ()\n","print (\"Target Language; index to word mapping\")\n","convert(targ_lang, target_tensor_train[0])"]},{"cell_type":"markdown","metadata":{},"source":["# Creating Tensorflow dataset"]},{"cell_type":"code","execution_count":15,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["(TensorShape([64, 24]), TensorShape([64, 24]))"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["BUFFER_SIZE = len(input_tensor_train)\n","BATCH_SIZE = 64\n","steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n","embedding_dim = 256\n","units = 1024\n","vocab_inp_size = len(inp_lang.word_index)+1\n","vocab_tar_size = len(targ_lang.word_index)+1\n","\n","dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n","dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n","\n","example_input_batch, example_target_batch = next(iter(dataset))\n","example_input_batch.shape, example_target_batch.shape"]},{"cell_type":"markdown","metadata":{},"source":["The TensorFlow dataset is created from the tokenized training data, shuffling it for randomness and batching it into manageable chunks of data. The example_input_batch and example_target_batch variables showcase the shape of a sample batch for input and output, providing an overview of the processed data structure. This preparation step ensures that the data is organized into batches suitable for training the chatbot model effectively."]},{"cell_type":"markdown","metadata":{},"source":["# Encoder/Decoder with attention equations"]},{"cell_type":"markdown","metadata":{},"source":["## Encoder"]},{"cell_type":"code","execution_count":19,"metadata":{"trusted":true},"outputs":[],"source":["class Encoder(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n","        super(Encoder, self).__init__()\n","        self.batch_sz = batch_sz\n","        self.enc_units = enc_units\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","        self.gru = tf.keras.layers.GRU(self.enc_units,\n","                                       return_sequences=True,\n","                                       return_state=True,\n","                                       recurrent_initializer='glorot_uniform')\n","\n","    def call(self, x, hidden):\n","        x = self.embedding(x)\n","        output, state = self.gru(x, initial_state = hidden)\n","        return output, state\n","\n","    def initialize_hidden_state(self):\n","        return tf.zeros((self.batch_sz, self.enc_units))"]},{"cell_type":"markdown","metadata":{},"source":[" The Encoder takes input sequences and processes them through an embedding layer, followed by a Gated Recurrent Unit (GRU) layer. The GRU layer's parameters include the number of encoder units and initialization settings. The `call` method processes the input through the embedding and GRU layers, returning the output sequences and the hidden states. Additionally, the class includes a method `initialize_hidden_state()` that initializes the hidden states for the GRU layer. This class encapsulates the functionality needed to encode input sequences into meaningful representations, a fundamental step in sequence-to-sequence models like chatbots."]},{"cell_type":"code","execution_count":20,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Encoder output shape: (batch size, sequence length, units) (64, 24, 1024)\n","Encoder Hidden state shape: (batch size, units) (64, 1024)\n"]}],"source":["encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n","\n","# sample input\n","sample_hidden = encoder.initialize_hidden_state()\n","sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n","print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n","print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"]},{"cell_type":"markdown","metadata":{},"source":["A sample input batch is processed to demonstrate the encoder's output shape, displaying the dimensions of the output sequence and the hidden state. This Encoder class is a fundamental component of the chatbot model, responsible for encoding input sentences into meaningful numerical representations."]},{"cell_type":"markdown","metadata":{},"source":["## Bahdanau Attention"]},{"cell_type":"markdown","metadata":{},"source":["This code defines a BahdanauAttention layer, a critical component in our chatbot application. "]},{"cell_type":"code","execution_count":21,"metadata":{"trusted":true},"outputs":[],"source":["class BahdanauAttention(tf.keras.layers.Layer):\n","    def __init__(self, units):\n","        super(BahdanauAttention, self).__init__()\n","        self.W1 = tf.keras.layers.Dense(units)\n","        self.W2 = tf.keras.layers.Dense(units)\n","        self.V = tf.keras.layers.Dense(1)\n","\n","    def call(self, query, values):\n","        query_with_time_axis = tf.expand_dims(query, 1)\n","        score = self.V(tf.nn.tanh(\n","            self.W1(query_with_time_axis) + self.W2(values)))\n","\n","        # attention_weights shape == (batch_size, max_length, 1)\n","        attention_weights = tf.nn.softmax(score, axis=1)\n","\n","        # context_vector shape after sum == (batch_size, hidden_size)\n","        context_vector = attention_weights * values\n","        context_vector = tf.reduce_sum(context_vector, axis=1)\n","\n","        return context_vector, attention_weights"]},{"cell_type":"markdown","metadata":{},"source":["The code defines a BahdanauAttention layer. The layer calculates attention weights for a given query and a set of values. It employs learnable parameters, W1, W2, and V, to compute a score, and then applies the softmax function to obtain attention weights. These weights indicate how much focus should be given to each element in the input sequence (values) when generating an output. "]},{"cell_type":"code","execution_count":22,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Attention result shape: (batch size, units) (64, 1024)\n","Attention weights shape: (batch_size, sequence_length, 1) (64, 24, 1)\n"]}],"source":["attention_layer = BahdanauAttention(10)\n","attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n","\n","print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n","print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"]},{"cell_type":"markdown","metadata":{},"source":["The layer also computes a context vector by weighing the values based on attention and reduces it to match the hidden state dimension. In the code, an instance of the BahdanauAttention layer is created and applied to sample hidden and output sequences, demonstrating the resulting attention result and attention weights shapes. This attention mechanism is crucial for the chatbot model to pay varying degrees of attention to different parts of the input sequence during response generation."]},{"cell_type":"markdown","metadata":{},"source":["## Decoder"]},{"cell_type":"markdown","metadata":{},"source":["The code here defines a Decoder class for the chatbot model, responsible for generating output sequences. "]},{"cell_type":"code","execution_count":30,"metadata":{"trusted":true},"outputs":[],"source":["class Decoder(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n","        super(Decoder, self).__init__()\n","        self.batch_sz = batch_sz\n","        self.dec_units = dec_units\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","        self.gru = tf.keras.layers.GRU(self.dec_units,\n","                                       return_sequences=True,\n","                                       return_state=True,\n","                                       recurrent_initializer='glorot_uniform')\n","        self.fc = tf.keras.layers.Dense(vocab_size)\n","\n","        # used for attention\n","        self.attention = BahdanauAttention(self.dec_units)\n","\n","    def call(self, x, hidden, enc_output):\n","        context_vector, attention_weights = self.attention(hidden, enc_output)\n","        x = self.embedding(x)\n","        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n","        output, state = self.gru(x)\n","        # output shape == (batch_size * 1, hidden_size)\n","        output = tf.reshape(output, (-1, output.shape[2]))\n","        # output shape == (batch_size, vocab)\n","        x = self.fc(output)\n","\n","        return x, state, attention_weights"]},{"cell_type":"markdown","metadata":{},"source":["The Decoder takes input tokens, hidden states, and encoder outputs. It first computes attention weights using the `BahdanauAttention` mechanism to focus on specific parts of the input sequence. Then, it processes the input tokens through an embedding layer and concatenates them with the context vector obtained from attention. The resulting sequence is passed through a GRU layer and a dense (fully connected) layer to produce the final output tokens."]},{"cell_type":"code","execution_count":31,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Decoder output shape: (batch_size, vocab size) (64, 2359)\n"]}],"source":["decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n","\n","sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n","                                      sample_hidden, sample_output)\n","\n","print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"]},{"cell_type":"markdown","metadata":{},"source":[" The model's capability is demonstrated by generating a sample output sequence, showcasing the Decoder's ability to predict responses based on the provided input and the learned patterns from the attention mechanism. This Decoder is a key component in the sequence-to-sequence architecture, enabling the chatbot to generate contextually relevant and coherent responses during conversations."]},{"cell_type":"markdown","metadata":{},"source":["# Training\n","\n","Here, training of the model is done so it generates responses for questions asked by the user.\n","\n","In the chatbot model, the conversation generation process involves several key steps. First, the input text is processed through the encoder, generating the encoder output and hidden state. Next, the encoder output, encoder hidden state, and the decoder input (usually the start token) are passed to the decoder. The decoder then predicts the next word and provides a decoder hidden state. This hidden state is fed back into the model, and the predictions are compared to the actual target words to calculate the loss. To guide the learning process, teacher forcing is employed, where the actual target word is fed as the input to the decoder instead of the previously predicted word. This technique helps in training the model more efficiently. Finally, gradients are calculated, applied to the optimizer, and utilized for backpropagation, enabling the model to iteratively learn and improve its conversational responses. These steps form a fundamental framework for training sequence-to-sequence models like chatbots."]},{"cell_type":"code","execution_count":32,"metadata":{"trusted":true},"outputs":[],"source":["optimizer = tf.keras.optimizers.Adam()\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True, reduction='none')\n","\n","def loss_function(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    loss_ = loss_object(real, pred)\n","\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","\n","    return tf.reduce_mean(loss_)"]},{"cell_type":"markdown","metadata":{},"source":["In this code snippet, an Adam optimizer is initialized along with a Sparse Categorical Crossentropy loss function. The loss function is customized to compute the loss only for valid tokens in sequences, ensuring efficient training."]},{"cell_type":"code","execution_count":33,"metadata":{"trusted":true},"outputs":[],"source":["@tf.function\n","def train_step(inp, targ, enc_hidden):\n","    loss = 0\n","\n","    with tf.GradientTape() as tape:\n","        enc_output, enc_hidden = encoder(inp, enc_hidden)\n","\n","        dec_hidden = enc_hidden\n","\n","        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n","\n","        # Teacher forcing - feeding the target as the next input\n","        for t in range(1, targ.shape[1]):\n","            # passing enc_output to the decoder\n","            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n","\n","            loss += loss_function(targ[:, t], predictions)\n","\n","            # using teacher forcing\n","            dec_input = tf.expand_dims(targ[:, t], 1)\n","\n","    batch_loss = (loss / int(targ.shape[1]))\n","\n","    variables = encoder.trainable_variables + decoder.trainable_variables\n","\n","    gradients = tape.gradient(loss, variables)\n","\n","    optimizer.apply_gradients(zip(gradients, variables))\n","\n","    return batch_loss"]},{"cell_type":"markdown","metadata":{},"source":["The `train_step` function is defined to execute one step of training. Within this function, the encoder processes input sequences to generate encoder output and hidden states."]},{"cell_type":"code","execution_count":35,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch:  4 Loss:1.5319\n","Epoch:  8 Loss:1.2881\n","Epoch: 12 Loss:1.0993\n","Epoch: 16 Loss:0.9390\n","Epoch: 20 Loss:0.7817\n","Epoch: 24 Loss:0.6069\n","Epoch: 28 Loss:0.4114\n","Epoch: 32 Loss:0.2315\n","Epoch: 36 Loss:0.1036\n","Epoch: 40 Loss:0.0454\n"]}],"source":["EPOCHS = 40\n","\n","for epoch in range(1, EPOCHS + 1):\n","    enc_hidden = encoder.initialize_hidden_state()\n","    total_loss = 0\n","\n","    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n","        batch_loss = train_step(inp, targ, enc_hidden)\n","        total_loss += batch_loss\n","\n","    if(epoch % 4 == 0):\n","        print('Epoch:{:3d} Loss:{:.4f}'.format(epoch,\n","                                          total_loss / steps_per_epoch))"]},{"cell_type":"markdown","metadata":{},"source":["The training loop runs for 40 epochs, where the model's performance is evaluated and printed every 4 epochs, showing the training loss. This entire process trains the chatbot model, enabling it to learn from the dataset and improve its conversational responses over multiple epochs."]},{"cell_type":"markdown","metadata":{},"source":["# Testing\n","\n","here, the testing begins to see if our chatbot generates responses."]},{"cell_type":"code","execution_count":36,"metadata":{"trusted":true},"outputs":[],"source":["def remove_tags(sentence):\n","    return sentence.split(\"<start>\")[-1].split(\"<end>\")[0]"]},{"cell_type":"code","execution_count":38,"metadata":{"trusted":true},"outputs":[],"source":["def evaluate(sentence):\n","    sentence = preprocess_sentence(sentence)\n","\n","    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n","    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n","                                                           maxlen=max_length_inp,\n","                                                           padding='post')\n","    inputs = tf.convert_to_tensor(inputs)\n","\n","    result = ''\n","    hidden = [tf.zeros((1, units))]\n","    enc_out, enc_hidden = encoder(inputs, hidden)\n","    dec_hidden = enc_hidden\n","    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n","\n","    for t in range(max_length_targ):\n","        predictions, dec_hidden, attention_weights = decoder(dec_input,\n","                                                             dec_hidden,\n","                                                             enc_out)\n","\n","        # storing the attention weights to plot later on\n","        attention_weights = tf.reshape(attention_weights, (-1, ))\n","\n","        predicted_id = tf.argmax(predictions[0]).numpy()\n","\n","        result += targ_lang.index_word[predicted_id] + ' '\n","\n","        if targ_lang.index_word[predicted_id] == '<end>':\n","            break\n","\n","        # the predicted ID is fed back into the model\n","        dec_input = tf.expand_dims([predicted_id], 0)\n","\n","    return remove_tags(result), remove_tags(sentence)"]},{"cell_type":"markdown","metadata":{},"source":["The evaluate function preprocesses an input sentence and generates a chatbot response using the trained encoder and decoder models. It tokenizes the input, processes it through the encoder, and iteratively predicts words using the decoder. The function stops when the \"end\" token is generated. The remove_tags function removes special tokens (\"start\" and \"end\") from the output, providing a clean response. This function demonstrates how the chatbot generates contextually relevant replies given user input, showcasing the model's conversational abilities."]},{"cell_type":"code","execution_count":39,"metadata":{"trusted":true},"outputs":[],"source":["def ask(sentence):\n","    result, sentence = evaluate(sentence)\n","\n","    print('Question: %s' % (sentence))\n","    print('Predicted answer: {}'.format(result))"]},{"cell_type":"code","execution_count":40,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Question:  good luck with school . \n","Predicted answer: thank you very much . \n","Question:  how are you \n","Predicted answer: i got lucky . \n"]}],"source":["ask(questions[10])\n","ask(\"how are you\")"]},{"cell_type":"markdown","metadata":{},"source":["The `ask` function utilizes the evaluate function to generate responses from the trained chatbot model. It takes a user input sentence, processes it, and prints both the input question and the predicted answer. This demonstrates what our chatbot will be generating on an given input.\n","\n","This shows that the model is now working."]},{"cell_type":"markdown","metadata":{},"source":["# The Website\n","\n","The `flask` framework integrates our html file with our model to provide an interactive chat application.\n","Once we execute `app.py` a server is created at port 5000 that shows the chat application webpage. \n"]},{"attachments":{"image.png":{"image/png":"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"}},"cell_type":"markdown","metadata":{},"source":["![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{},"source":["The webpage provides a clean,minimalistic user interface to interact with our chatbot"]},{"cell_type":"markdown","metadata":{},"source":["When the user accesses the home route (\"/\"), it renders an HTML template, typically containing an input field and a submit button for users to interact with the chatbot. The \"/get\" route is designed to receive user input via a GET request, extract the user's message, pass it to a chatbot model for prediction, and return the predicted response as a JSON string."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
